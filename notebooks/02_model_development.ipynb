{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Model Development for Molecular Solubility Prediction\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates the model development process for predicting molecular solubility.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We'll perform the following tasks:\\n\",\n",
    "    \"1. Load the processed data with molecular descriptors\\n\",\n",
    "    \"2. Split the data into training and testing sets\\n\",\n",
    "    \"3. Train multiple models and optimize hyperparameters\\n\",\n",
    "    \"4. Evaluate model performance\\n\",\n",
    "    \"5. Analyze feature importance\\n\",\n",
    "    \"6. Make predictions on new compounds\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import libraries\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from rdkit import Chem\\n\",\n",
    "    \"from rdkit.Chem import Draw\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import scikit-learn modules\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestRegressor\\n\",\n",
    "    \"from sklearn.linear_model import Ridge, Lasso\\n\",\n",
    "    \"from sklearn.ensemble import GradientBoostingRegressor\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import from our own modules\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"from src.models.train_model import train_test_split_data, train_random_forest, evaluate_model\\n\",\n",
    "    \"from src.visualization.visualize import plot_feature_importance, plot_actual_vs_predicted, plot_distributions\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set up plotting\\n\",\n",
    "    \"%matplotlib inline\\n\",\n",
    "    \"plt.style.use('seaborn-whitegrid')\\n\",\n",
    "    \"sns.set_palette('viridis')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load Processed Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's load the processed data we created in the previous notebook.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Load the processed data\\n\",\n",
    "    \"features_df = pd.read_csv('../data/processed/molecular_features.csv')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display the first few rows\\n\",\n",
    "    \"features_df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Check the shape of the features dataframe\\n\",\n",
    "    \"print(f\\\"Feature dataframe shape: {features_df.shape}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check for missing values\\n\",\n",
    "    \"missing_values = features_df.isnull().sum()\\n\",\n",
    "    \"print(\\\"\\\\nFeatures with missing values:\\\")\\n\",\n",
    "    \"print(missing_values[missing_values > 0] if any(missing_values > 0) else \\\"None\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Prepare Data for Modeling\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now we'll split the data into features (X) and target (y), and then into training and testing sets.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Separate features and target\\n\",\n",
    "    \"X = features_df.drop(['SMILES', 'logS_exp'], axis=1)\\n\",\n",
    "    \"y = features_df['logS_exp']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply feature scaling\\n\",\n",
    "    \"scaler = StandardScaler()\\n\",\n",
    "    \"X_scaled = scaler.fit_transform(X)\\n\",\n",
    "    \"X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split data into training and testing sets\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split_data(X_scaled, y, test_size=0.2, random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training set shape: {X_train.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Testing set shape: {X_test.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Baseline Models\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's train and evaluate several baseline models to establish a performance benchmark.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Define models to evaluate\\n\",\n",
    "    \"models = {\\n\",\n",
    "    \"    'Ridge': Ridge(),\\n\",\n",
    "    \"    'Lasso': Lasso(),\\n\",\n",
    "    \"    'Random Forest': RandomForestRegressor(random_state=42),\\n\",\n",
    "    \"    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train and evaluate each model using cross-validation\\n\",\n",
    "    \"cv_results = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for name, model in models.items():\\n\",\n",
    "    \"    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\\n\",\n",
    "    \"    rmse_scores = np.sqrt(-cv_scores)\\n\",\n",
    "    \"    cv_results[name] = {\\n\",\n",
    "    \"        'mean_rmse': rmse_scores.mean(),\\n\",\n",
    "    \"        'std_rmse': rmse_scores.std()\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    print(f\\\"{name}: Mean RMSE = {rmse_scores.mean():.4f} (Â±{rmse_scores.std():.4f})\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Visualize cross-validation results\\n\",\n",
    "    \"cv_df = pd.DataFrame({\\n\",\n",
    "    \"    'Model': list(cv_results.keys()),\\n\",\n",
    "    \"    'Mean RMSE': [result['mean_rmse'] for result in cv_results.values()],\\n\",\n",
    "    \"    'Std RMSE': [result['std_rmse'] for result in cv_results.values()]\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"sns.barplot(x='Model', y='Mean RMSE', data=cv_df)\\n\",\n",
    "    \"plt.errorbar(\\n\",\n",
    "    \"    x=np.arange(len(cv_df)),\\n\",\n",
    "    \"    y=cv_df['Mean RMSE'],\\n\",\n",
    "    \"    yerr=cv_df['Std RMSE'],\\n\",\n",
    "    \"    fmt='none',\\n\",\n",
    "    \"    color='black',\\n\",\n",
    "    \"    capsize=5\\n\",\n",
    "    \")\\n\",\n",
    "    \"plt.title('Cross-Validation RMSE for Different Models')\\n\",\n",
    "    \"plt.ylabel('RMSE')\\n\",\n",
    "    \"plt.grid(axis='y', alpha=0.3)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Hyperparameter Tuning\\n\",\n",
    "    \"\\n\",\n",
    "    \"Based on the baseline results, let's optimize the hyperparameters of the best-performing model.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Define parameter grid for Random Forest\\n\",\n",
    "    \"rf_param_grid = {\\n\",\n",
    "    \"    'n_estimators': [100, 200, 300],\\n\",\n",
    "    \"    'max_depth': [None, 10, 20, 30],\\n\",\n",
    "    \"    'min_samples_split': [2, 5, 10],\\n\",\n",
    "    \"    'min_samples_leaf': [1, 2, 4]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train Random Forest with hyperparameter tuning\\n\",\n",
    "    \"rf_model = train_random_forest(X_train, y_train, param_grid=rf_param_grid)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Model Evaluation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now let's evaluate the optimized model on the test set.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Evaluate the optimized model\\n\",\n",
    "    \"metrics = evaluate_model(rf_model, X_test, y_test)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Make predictions on the test set\\n\",\n",
    "    \"y_pred = rf_model.predict(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot actual vs predicted values\\n\",\n",
    "    \"fig = plot_actual_vs_predicted(y_test, y_pred)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Plot distributions of actual and predicted values\\n\",\n",
    "    \"fig = plot_distributions(y_test, y_pred)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Feature Importance Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's analyze which molecular descriptors are most important for solubility prediction.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Plot feature importance\\n\",\n",
    "    \"fig = plot_feature_importance(rf_model, X.columns, top_n=15)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Prediction on New Compounds\\n\",\n",
    "    \"\\n\",\n",
    "    \"Finally, let's demonstrate how to use the trained model to predict solubility for new compounds.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import prediction module\\n\",\n",
    "    \"from src.models.predict_model import predict_solubility\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Example SMILES for new compounds\\n\",\n",
    "    \"new_smiles = [\\n\",\n",
    "    \"    'CC(=O)OC1=CC=CC=C1C(=O)O',  # Aspirin\\n\",\n",
    "    \"    'CN1C=NC2=C1C(=O)N(C(=O)N2C)C',  # Caffeine\\n\",\n",
    "    \"    'CCC1(CC)C(=O)NC(=O)N(C)C1=O',  # Phenobarbital\\n\",\n",
    "    \"    'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O',  # Ibuprofen\\n\",\n",
    "    \"    'CN1C2CCC1CC(C2)OC(=O)C(CO)C3=CC=CC=C3'  # Cocaine\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Make predictions\\n\",\n",
    "    \"predictions = predict_solubility(rf_model, new_smiles, scaler=scaler)\\n\",\n",
    "    \"predictions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Visualize the predicted compounds\\n\",\n",
    "    \"mols = [Chem.MolFromSmiles(smiles) for smiles in predictions['SMILES']]\\n\",\n",
    "    \"legends = [f\\\"{Chem.MolToSmiles(mol, isomericSmiles=False)}\\\\nLogS: {pred:.2f}\\\" \\n\",\n",
    "    \"           for mol, pred in zip(mols, predictions['Predicted_Solubility'])]\\n\",\n",
    "    \"\\n\",\n",
    "    \"img = Draw.MolsToGridImage(mols, molsPerRow=3, subImgSize=(300, 300), legends=legends)\\n\",\n",
    "    \"display(img)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Save Model and Scaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's save the trained model and scaler for future use.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import function to save model\\n\",\n",
    "    \"from src.models.train_model import save_model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create directories if they don't exist\\n\",\n",
    "    \"os.makedirs('../models', exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save model\\n\",\n",
    "    \"save_model(rf_model, '../models/random_forest_model.pkl')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save scaler\\n\",\n",
    "    \"save_model(scaler, '../models/scaler.pkl')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook, we have:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. Loaded the processed molecular descriptor data\\n\",\n",
    "    \"2. Prepared the data for modeling by scaling features and splitting into training/testing sets\\n\",\n",
    "    \"3. Trained and evaluated several baseline models\\n\",\n",
    "    \"4. Optimized the hyperparameters of the Random Forest model\\n\",\n",
    "    \"5. Evaluated the optimized model on the test set\\n\",\n",
    "    \"6. Analyzed feature importance to understand which descriptors are most predictive\\n\",\n",
    "    \"7. Demonstrated how to make predictions on new compounds\\n\",\n",
    "    \"8. Saved the trained model and scaler for future use\\n\",\n",
    "    \"\\n\",\n",
    "    \"The final model achieved good performance in predicting molecular solubility based on chemical descriptors calculated from molecular structures. This demonstrates a complete machine learning workflow for structure-property relationship modeling.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
